---
title: "Statistik"
author: "CH.13 - Multiple Regression"
date: "SS 2022 || Prof. Dr. Buchwitz, Sommer, Henke"

toc: true
tocoverview: false
tocheader: Inhaltsübersicht
titlefontsize: 22pt

output: fhswf::presentation
knit: rmarkdown::render
---

```{r setup, include=FALSE}
library(knitr)
options(digits = 4)
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE, 
                      dev.args=list(pointsize=11), size="scriptsize")

# Packages
library(car)
library(fGarch)
library(gtools)
library(HistogramTools)
library(ineq)
library(kableExtra)
library(texreg)
library(tidyverse)
library(vcd)
```

## Lernziele

- Erweitern des Prinzips der einfachen linearen Regression auf mehrere Variablen.
- Unterscheidung zwischen und Test von Partial- und aggregierten Effekten mittels Hypothesentest.
- Identifikation von und Umgang mit speziellen Problemen (Residualannahmen, Multikollinearität, Nichtlinearität) im Kontext der Regression.

# Multiple Lineare Regression

## Multiple Lineare Regression

\Large 
$$ Y = f(X_1,X_2,\ldots,X_p) + \epsilon$$
\bigskip
\normalsize

- Mit Hilfe der **einfachen linearen Regression** kann der Zusammenhang einer abhängigen Variablen $Y$ mit **einer** unabhängigen Variablen $X$ modelliert werden.
- Die **multiple lineare Regression** erlaubt das Modellieren des Zusammenhangs einer abhängigen Variablen $Y$ mit **mehreren** unabhängigen Variablen $X_1,X_2,\ldots, X_p$.

## Multiple Lineare Regression

\Large
$$ Y = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon$$
\bigskip
\normalsize

- Die zuvor diskutierte einfache lineare Regression kann als **Spezialfall** der multiplen linearen Regression aufgefasst werden, bei der gilt $p=1$. 
- Wir nehmen weiterhin an, dass *innerhalb des Wertebereichs* der Daten der wahre Zusammenhang zwischen $Y$ und den Prädiktoren durch ein lineares Modell **approximiert** werden kann.
- Jeder Regressor geht mit einem eigenen Koeffizienten $\beta_1,  \beta_2, \ldots, \beta_p$ in die Gleichung ein. Der Fehlerterm $\epsilon$ enthält zudem keine **systematischen Informationen** zur Erklärung der Streuung von $Y$, die nicht bereits durch die Regressoren abgebildet wurden.

## Multiple Lineare Regression

\Large
$$ y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_p x_{ip} + \epsilon_i$$
\normalsize
\smallskip
$$ i=1,2,\ldots,n$$

\bigskip
- Aus der Modellgleichung folgt die obige Darstellung für jede Beobachtung. Dabei repräsentiert $y_i$ die $i$-te Beobachtung der abhängigen Variablen $Y$. Die Werte $x_{i1},x_{i2},\ldots,x_{ip}$ sind die Werte der zugehörigen Regressoren für die $i$-te Beobachtung in der Stichprobe (üblicherweise $i$-te Zeile im Datensatz).
- Der Wert $\epsilon_i$ ist der Anpassungsfehler (Fehlerterm) der linearen Approximation für die $i$-te Beobachtungseinheit.

## Beispiel: Autodaten

```{r, echo=F}
d <- mtcars
d$l100km <- 235.215/mtcars$mpg
d$weight <- d$wt * 0.453592 
d$hub <- d$disp * 0.0163871
d <- d[,c("l100km", "weight", "hp", "cyl", "hub")]
```

```{r, size="tiny"}
d
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Datenbeschreibung}
\texttt{l100km}\; Kraftstoffverbrauch in Litern pro 100km bei normaler Fahrweise.\\
\texttt{weight}\; Fahrzeuggewicht in Tonnen.\\
\texttt{hp}\; Motorleistung in PS.\\
\texttt{cyl}\; Anzahl der Zylinder des Fahrzeugmotors.\\
\texttt{hub}\; Hubraum des Motors in Litern.\\
\end{alertblock}
\end{textblock}


## Beispiel: Autodaten

```{r}
dim(d)                # Anzahl Beobachtungen und Anzahl Variablen
t(sapply(d, summary)) # Deskriptive Statistik für alle Variablen
```

## Beispiel: Autodaten

```{r}
round(var(d),4)       # Varianz-Kovarianz-Matrix
round(cor(d),4)       # Paarweise Korrelationskoeffizienten
```

## Multiple Lineare Regression

Wovon ist die Größe `l100km` abhängig?
\kariert{18}

## Multiple Lineare Regression

$$ 
\begin{array}{ccccccccc}
Y                          &=& \beta_0 &+& \beta_1  X_1            &+& \beta_2 X_2   &+& \epsilon\\
\text{Kraftstoffverbrauch} &=& \beta_0 &+& \beta_1  \text{Gewicht} &+& \beta_2 \text{Motorleistung} &+& \epsilon
\end{array}
$$

\bigskip
- Wir nehmen an, dass $Y$ linear von (mindestens) zwei erklärenden Variablen abhängig ist. 
- Diese Annahme muss verifiziert werden (was wir zunächst ignorieren), da sonst nicht sichergestellt ist, dass diese Variablen einen Einfluss haben oder entscheidende Variablen im Modell fehlen.

\pause
\begin{alertblock}{}
\Large \center
Wie können die Parameter $\beta_0, \beta_1, \ldots, \beta_p$ bei der multiplen linearen Regression bestimmt werden?
\end{alertblock}

## Parameterschätzung

Wie können die Regressionsparameter $\beta_0, \beta_1, \ldots, \beta_p$ bestimmt werden?
\kariert{18}

## Parameterschätzung

- **Lösung:** Minimieren der Fehlerquadratsumme nach dem Prinzip der kleinsten Quadrate (Kleinste-Quadrate-Schätzung).
\bigskip
- Der Anpassungsfehler für jede Beobachtung ergibt sich aus der umgestellten Beobachtungsgleichung:
$$\epsilon_i = y_i - \beta_0 - \beta_1 x_{i1} - \beta_2 x_{i2} - \ldots - \beta_p x_{ip}$$

- Die zu minimierende Funktion in Abhängigkeit der Parameter $\beta_0, \beta_1, \ldots, \beta_p$ ergibt sich damit wie folgt:

$$S(\beta_0, \beta_1, \ldots, \beta_p) = \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_{i1} - \beta_2 x_{i2} - \ldots - \beta_p x_{ip})^2$$

- $\hat\beta_0, \hat\beta_1, \ldots, \hat\beta_p$ bzw. $b_0, b_1, \ldots, b_p$ sind die Werte, die die Funktion $S(\;)$ minimieren.

## Parameterschätzung

```{r}
mod <- lm(l100km ~ 1 + weight + hp, data=d)
mod
```

- Multiple lineare Regressionsmodelle können in R ebenfalls mit Hilfe der Funktion `lm()` geschätzt werden.
- R greift für die Bestimmung der Parameterschätzer ebenfalls auf die Methode der kleinsten Quadrate zurück. 

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your Turn}
Schreiben Sie die zugehörige Regressionsgleichung auf.
\end{alertblock}
\end{textblock}

## Darstellung und Interpretation

- Einfache Regressionsmodelle (nur $X_1$) können als Gerade dargestellt werden. Multiple Regressionsmodelle ($X_1$ und $X_2$) können mit einer Ebene oder als Hyperebene (mehr als zwei Prädiktoren) dargestellt werden. Diese Darstellung wird sehr schnell unübersichtlich.
- $\beta_0$ ist der Achsenabschnitt und der abgebildete Wert von $Y$, wenn $X_1 = X_2 = \ldots = X_p = 0$.
- Die Steigungskoeffizienten $\beta_j$ haben mehrere Interpretationen:
  + $\beta_j$ ist die **Veränderung** in $Y$, wenn sich $X_j$ um eine Einheit erhöht und alle anderen Prädiktoren konstant gehalten werden (ceteris paribus).
  + $\beta_j$ wird also als **Partialeffekt** bezeichnet, weil er den Effekt von $X_j$ auf $Y$ abbildet, nachdem die Zielvariable um die Effekte der anderen Variablen adjustiert wurde.

## Darstellung und Interpretation

```{r, size="tiny"}
mod <- lm(l100km ~ 1 + weight + hp, data=d)
summary(mod)
```

\begin{textblock}{4.6}(7.5,1)
\begin{alertblock}{Your Turn}
Interpretieren Sie die Schätzergebnisse ($\alpha=0.05$) und das Gütemaß des Regressionsmodells.  
\end{alertblock}
\end{textblock}

## Darstellung und Interpretation

- In wissenschaftlichen Aufsätzen werden Regressionsmodelle häufig schrittweise aufgebaut und übersichtlich in Tabellen dargestellt.

\bigskip

```{r, results='asis', echo=F}
mod_1 <- lm(l100km ~ 1 + weight, data=d)
mod_2 <- lm(l100km ~ 1 + hp    , data=d)
mod_3 <- lm(l100km ~ 1 + hp + weight, data=d)
texreg(list(mod_1,mod_2,mod_3))
```

# Hypothesentests

## Hypothesentests

- Ergänzend zum t-Test für die einzelnen Koeffizienten ($H_0: \beta_j = 0$ vs. $H_1: \beta_j \neq 0$) gibt es auch die Möglichkeit, **alle Koeffizienten auf einmal** einem Hypothesentest zu unterziehen.
- Das Szenario, ob alle Regressoren zusammen genommen einen Effekt auf die abhängige Variable $Y$ haben, kann mit Hilfe des **F-Tests** untersucht werden.
- Die Idee dieses simultanen Testens ist, zu prüfen, ob mit hoher Wahrscheinlichkeit davon auszugehen ist, dass nicht alle Parameter $\beta_1, \beta_2, \ldots, \beta_p$ gleich 0 sind, also zu prüfen:

$$
H_0: \beta_1 = \beta_2 = \ldots = \beta_p = 0 \quad\text{vs}\quad H_1: \beta_j \neq 0 \;\text{für min. ein}\; j
$$

## Hypothesentests

$$
\begin{array}{cl}
\text{FM:}\quad & Y = \beta_0 + \beta_1X_1 + \beta_2 X_2 + \ldots + \beta_p X_p + \epsilon \\
\text{RM:}\quad & Y = \beta_0 + \epsilon
\end{array}
$$

\bigskip
- Die Nullhypothese ist gleichbedeutend mit der Tatsache, dass auch ein **reduziertes Modell** (RM) ohne Regressoren den gleichen Erklärungsgehalt liefert wie das volle Modell (FM) mit allen $p$ Regressoren.
- Dieser fehlende **Fit** kann mit Hilfe der Fehlerquadratsumme (SSE) für die beiden Modelle messbar gemacht werden. 

$$SSE(FM) = \sum(y_i - \hat y_i)^2 \quad\quad SSE(RM) = \sum(y_i - \hat y_i^*)^2$$

## Hypothesentests

$$ F = \frac{[SSE(RM) - SSE(FM)]/(p+1-k)}{SSE(FM)/(n-p-1)}$$

\bigskip
- Die Differenz $SSE(RM) - SSE(FM)$ gibt die Erhöhung der Residualstreuung durch Rückgriff auf das reduzierte Model an. Wenn diese Differenz groß ist, ist das RM mit $k$ Parametern **nicht adäquat**.
- Wenn der beobachtete F-Wert größer ist als der kritische Wert, ist der F-Test signifikant zum Level  $\alpha$.
- Das bedeutet, dass das reduzierte Modell (RM) nicht zufriedenstellend ist und die Nullhypothese (und die entsprechenden Werte für die $\beta$'s) verworfen werden kann.
- Verwerfe $H_0$, wenn gilt:
$$F \geq F_{(p+1-k, \; n-p-1 \;;1-\alpha)} \quad\text{oder}\quad p(F)\leq\alpha$$

## Hypothesentests

```{r, size="tiny"}
mod <- lm(l100km ~ 1 + weight + hp, data=d)
summary(mod)
```

# Residualdiagnostik

## Residualdiagnostik

- Modellierungsprobelme wie inkorrekt spezifizierte Modelle, fehlende und vergessene Variablen **äußern sich häufig in einer Verletzung der Annahmen der Residuen**.
- Um zu überprüfen, ob das ausgewählte Regressionsmodell den theoretischen Anforderungen genügt, müssen daher die Residuen inspiziert werden. Diesen Prozess nennt man Residualdiagnostik.
- Residuen sollten (annähernd) normalverteilt sein, keine Zusammenhangsstrutkur aufweisen (i.i.d.) und frei von Ausreißern sein. Diese Eigenschaften werden häufig in **grafischen Darstellungen** der Residuen sichtbar.
- **R-Funktion:** `residuals()` erlaubt das Extrahieren von Residuen aus der Rückgabe der `lm()` Funktion.

## Residualdiagnostik

- Darstellung Residuen der Regression Kraftstroffverbrauch $Y$ erklärt durch Fahrzeuggewicht ($X_1$) und Motorleistung ($X_2$).

$$
Y = \beta_0 + \beta_1  X_1 + \beta_2 X_2 + \epsilon
$$

```{r, fig.height=4, echo=F}
par(mfrow=c(1,2))
res <- residuals(mod)
plot(res, main="Indexplot der Residuen", ylab="", xlab="")
hist(res, main="Histogramm der Residuen", xlab="")
```

# Multikollinearität

## Multikollinearität

- Die Interpretation der Koeffizienten eines multiplen Regressionsmodells setzt voraus, dass die Prädiktoren keinen ausgeprägten Zusammenhang untereinander haben, da die (ceteris paribus) Interpretation der Koeffizienten dann nicht mehr greift.
- Wenn eine starke Abhängigkeitsstruktur zwischen den Prädiktoren vorhanden ist, dann bezeichnet man dieses Problem als **Multikollinearität**. Multikollinearität ist ein Problem in den Daten und kein Problem der Modellierung.
- Multikollinearität führt zu unplausiblen Werten der Koeffizientenschätzer und wird durch spezielle Maßzahlen wie Varianzinflationsfaktoren (VIF) messbar.

## Multikollinearität

```{r}
cor(d)
```

## Varianzinflationsfaktoren

- Um Multikollinearitätsprobleme zu diagnostizieren, müssen die Zusammenhangsstrukturen zwischen den Prädiktoren untersucht werden. Das beinhaltet die Analyse des $R^2$, das aus der Regression jedes Prädiktors auf alle verbleibenden Prädiktoren resultiert.

$$
\text{VIF}_j = \frac{1}{1-R^2_j} \quad\text{mit}\quad j=1,\ldots,p
$$

- $R^2_j$ bezeichnet das Bestimmtheitsmaß bei der Erklärung von $X_j$ durch alle verbleibenden $p-1$ Prädiktoren. Wenn $X_j$ gut durch die anderen Variablen erklärt werden kann, wird das $R^2_j$ nah bei 1 sein und in einem großen Wert des VIF$_j$ resultieren.

###
\center
Ein Wert von $\text{VIF}>10$ wird oft als Grenzwert gesehen, ab dem man von Multikollinearität in problematischem Ausmaß ausgeht.

## Varianzinflationsfaktoren

- Varianzinflationsfaktoren können mit der **R-Funktion** `vif()` aus dem Zusatzpaket `car` berechnet werden.

```{r}
mod1 <- lm(l100km ~ 1 + weight + hp, data=d)
car::vif(mod1)
mod2 <- lm(l100km ~ 1 + weight + hp + hub + cyl, data=d)
car::vif(mod2)
```

# Nichtlinearität

## Nichtlinearität

\Large 
$$ Y = \beta_0 + \beta_1X_1 + \beta_2 X_1^2 + \epsilon$$
\bigskip
\normalsize

- Die lineare Regression ist linear in Bezug auf die Tatsache, dass die Parameter $\beta_0, \beta_1, \ldots, \beta_p$ **linear** in das Modell eingehen. 
- Mit der linearen Regression können dennoch **nicht-lineare** Zusammenhänge modelliert werden, indem **nicht-lineare Transformationen** als zusätzliche unabhängige Variablen in das Modell integriert werden.

## Nichtlinearität

\center
**Welches Modell passt besser zu den gezeigten Daten?**
```{r, echo=F}
set.seed(1)
x <- runif(25,0,20) # Experience
x_sq <- x^2
cu <- function(x){25 + 1.5 * x + 0.25 * x^2}
y <- cu(x) + rnorm(length(x), sd=2.5) # Salary

mod1 <- lm(y ~ 1 + x)
mod2 <- lm(y ~ 1 + x + x_sq)

par(mfrow=c(1,2))                    
plot(x,y, xlim=c(0,20), ylim=range(y), xlab="Experience", ylab="Salary")
abline(mod1, col="red")

curve(cu, from = 0, to=25,xlim=c(0,20), ylim=range(y), type="l",
     col="red",  xlab="Experience", ylab="Salary")
points(x,y)
```

## Nichtlinearität

```{r}
mod1
mod2
```

## Verständnisfragen

- Wie ist das Bestimmtheitsmaß $R^2$ bei der multiplen Regression zu interpretieren?
- Was ist damit gemeint, dass die diskutierten Regressionsmodelle lineare Modelle sind?
- Was ist Multikollinearität?
